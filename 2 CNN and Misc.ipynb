{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These first 6 cells are concerned with converting raw image files to 4d tensors and creating the\n",
    "# train and test label dictionaries\n",
    "\n",
    "import os\n",
    "from skimage import io\n",
    "from skimage.color import rgb2gray\n",
    "from skimage.transform import resize\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_dir_train = 'D:/Deep-Learning/base_dir/train/both_resize/'\n",
    "img_dir_test = 'D:/Deep-Learning/base_dir/test/both_resize/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(input(\"Output Filename?: \"), 'w') as writer:\n",
    "    for filename in os.listdir(img_dir_train):\n",
    "        writer.write(filename + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter list file: D:/Deep-Learning/to.txt\n"
     ]
    }
   ],
   "source": [
    "with open(input(\"Enter list file: \"), 'r') as reader:\n",
    "    train_filenames = reader.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = []\n",
    "for i in range(len(train_filenames)):\n",
    "    if 'benign' in train_filenames[i]:\n",
    "        train_labels.append(0)\n",
    "    else:\n",
    "        train_labels.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels_dict = {}\n",
    "for i in range(len(train_filenames)):\n",
    "    train_labels_dict[train_filenames[i]] = int(train_labels[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(input(\"Train Filename: \"), 'r')\n",
    "lines = f.read().splitlines()\n",
    "f.close()\n",
    "\n",
    "train_labels_sk = {}\n",
    "for i in range(len(lines)):\n",
    "    temp = lines[i].split('\\t')\n",
    "    filename = temp[0]\n",
    "    Class = temp[1]\n",
    "    print(Class)\n",
    "    print(filename)\n",
    "    train_labels_sk[filename] = int(Class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#resizing images in bulk\n",
    "\n",
    "from PIL import Image\n",
    "import glob, os\n",
    "import numpy as np\n",
    "from numpy import random\n",
    "from random import *\n",
    "\n",
    "os.chdir(img_dir_test)\n",
    "\n",
    "#size = (150, 150)\n",
    "\n",
    "for infile in glob.glob('*.jpg'):\n",
    "    file, ext = os.path.splitext(infile)\n",
    "    im = Image.open(infile)\n",
    "    height = randint(10, 150)\n",
    "    width = randint(10, 150)\n",
    "    new_im = im.resize((height, width))\n",
    "    im.close()\n",
    "    new_im.save('D:/Deep-Learning/testing/' + file + '.jpg', 'JPEG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#imagedir = input(\"Train Image Directory: \")\n",
    "files = os.listdir(img_dir_train)\n",
    "\n",
    "I = io.imread(img_dir_train+files[0])\n",
    "x_train = np.empty(shape=(len(files), I.shape[0], I.shape[1], I.shape[2]), dtype=np.int)\n",
    "y_train = np.empty(shape=(len(files)), dtype=np.int)\n",
    "for i in range(len(files)):\n",
    "    I = io.imread(img_dir_train+files[i])\n",
    "    x_train[i,:,:,:] = I\n",
    "    y_train[i] = int(train_labels_dict[files[i]])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output Filename?: D:/Deep-Learning/to_test.txt\n"
     ]
    }
   ],
   "source": [
    "with open(input(\"Output Filename?: \"), 'w') as writer:\n",
    "    for filename in os.listdir(img_dir_test):\n",
    "        writer.write(filename + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter list file: D:/Deep-Learning/to_test.txt\n"
     ]
    }
   ],
   "source": [
    "with open(input(\"Enter list file: \"), 'r') as reader:\n",
    "    test_filenames = reader.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels = []\n",
    "for i in range(len(test_filenames)):\n",
    "    if 'benign' in test_filenames[i]:\n",
    "        test_labels.append(0)\n",
    "    else:\n",
    "        test_labels.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels_dict = {}\n",
    "for i in range(len(test_filenames)):\n",
    "    test_labels_dict[test_filenames[i]] = int(test_labels[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(input(\"Test Filename: \"), 'r')\n",
    "lines1 = f.read().splitlines()\n",
    "f.close()\n",
    "test_labels_sk = {}\n",
    "#print(\"processing class labels\")\n",
    "for i in range(0,len(lines1)):\n",
    "    tmp=lines1[i].split(\"\\t\")\n",
    "    filename=tmp[0]\n",
    "    Class=tmp[1]\n",
    "    test_labels_sk[filename]=int(Class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Image Directory: D:/Deep-Learning/base_dir/test/both_resize/\n"
     ]
    }
   ],
   "source": [
    "imagedir1= input(\"Test Image Directory: \")\n",
    "files1=os.listdir(imagedir1)\n",
    "#testing images\n",
    "I = io.imread(imagedir1+files1[0])\n",
    "x_test = np.empty(shape=(len(files1), I.shape[0],I.shape[1], I.shape[2]), dtype=np.int)\n",
    "#class labels of testing images\n",
    "y_test = np.empty(shape=(len(files1)),dtype=np.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done processing 0 images\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,len(files1)):\n",
    "    if(i%2000 == 0):\n",
    "        print('done processing ' + str(i) + ' images')\n",
    "    I = io.imread(imagedir1+files1[i])\n",
    "    x_test[i,:,:] = I\n",
    "    y_test[i] = int(test_labels_dict[files1[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save File: D:/Deep-Learning/150x150_ex_dataset.hdf5\n"
     ]
    }
   ],
   "source": [
    "hf = h5py.File(input(\"Save File: \"), 'w')\n",
    "hf.create_dataset('x_train', data=x_train)\n",
    "hf.create_dataset('y_train', data=y_train)\n",
    "hf.create_dataset('x_test', data=x_test)\n",
    "hf.create_dataset('y_test', data=y_test)\n",
    "hf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout, Dense, Conv2D, MaxPooling2D, BatchNormalization\n",
    "from keras.layers.core import Flatten\n",
    "from keras.models import load_model\n",
    "from keras.activations import *\n",
    "from keras.optimizers import *\n",
    "#import talos\n",
    "#import wrangle\n",
    "import numpy as np\n",
    "import h5py\n",
    "import os\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "\n",
    "\n",
    "callback_path = input(\"Callback Filename: \")    \n",
    "checkpoint = ModelCheckpoint(callback_path,monitor='val_loss',verbose=1,save_best_only=True)\n",
    "early_stopping_monitor = EarlyStopping(monitor='val_loss',patience=8)\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "filepath = input(\"Enter .hdf5 Filename to Load From: \")\n",
    "\n",
    "hf = h5py.File(filepath, 'r')\n",
    "\n",
    "x_train = np.array(hf['x_train'])\n",
    "\n",
    "y_train = np.array(hf['y_train'])\n",
    "\n",
    "x_test = np.array(hf['x_test'])\n",
    "\n",
    "y_test = np.array(hf['y_test'])\n",
    "\n",
    "hf.close()\n",
    "\n",
    "#x, y = x_train, y_train\n",
    "\n",
    "#x_train, y_train, x_val, y_val = wrangle.array_split(x_train, y_train, .2)\n",
    "\n",
    "#x_train, x_val, x_test = x_train/255, x_val/255, x_test/255\n",
    "\n",
    "dim1 = x_train.shape[1]\n",
    "\n",
    "dim2 = x_train.shape[2]\n",
    "\n",
    "dim3 = x_train.shape[3]\n",
    "\n",
    "INPUT_SHAPE = (dim1, dim2, dim3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myModel = Sequential()\n",
    "myModel.add(Dense(16, activation = 'relu', input_shape=(INPUT_SHAPE)))\n",
    "myModel.add(Flatten())\n",
    "myModel.add(Dense(1, activation = 'sigmoid'))\n",
    "myModel.compile(optimizer='Nadam', loss='binary_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = myModel.fit(x_train, y_train, validation_data=(x_val, y_val), epochs=100, verbose=1, batch_size = 25, callbacks=[checkpoint, early_stopping_monitor])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "filepath = input(\"Enter Filename: \")\n",
    "myModel = load_model(filepath)\n",
    "for i in range(0,len(myModel.layers)):\n",
    "    print(myModel.layers[i].name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('D:/Deep-Learning/')\n",
    "with open('train_benign.txt', 'w') as writer:\n",
    "    for file in os.listdir('D:/Deep-Learning/base_dir/train/benign/'):\n",
    "        writer.write(file + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, shutil\n",
    "\n",
    "for file in os.listdir('D:/ISBI-Data/ex_image_set/'):\n",
    "    shutil.copy('D:/ISBI-Data/ex_image_set/' + file, 'D:/ISBI-Data/ex_image_set/' + 'a' + file)\n",
    "    shutil.copy('D:/ISBI-Data/ex_image_set/' + file, 'D:/ISBI-Data/ex_image_set/' + 'b' + file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('D:/Deep-Learning/ex_dataset.txt', 'w') as writer:\n",
    "    for file in os.listdir('D:/ISBI-Data/ex_image_set/'):\n",
    "        writer.write(file + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('D:/Deep-Learning/ALL_TRAIN_ex_ground_truth.txt', 'r') as reader:\n",
    "    new_names = reader.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "i = 0\n",
    "\n",
    "for file in os.listdir('D:/ISBI-Data/ex_image_set/'):\n",
    "    os.rename('D:/ISBI-Data/ex_image_set/' + file, 'D:/ISBI-Data/ex_image_set/' + new_names[i])\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout, Dense, Conv2D, MaxPooling2D, BatchNormalization\n",
    "from keras.layers.core import Flatten\n",
    "from keras.models import load_model\n",
    "from keras.activations import *\n",
    "from keras.optimizers import *\n",
    "#import talos\n",
    "#import wrangle\n",
    "import numpy as np\n",
    "import h5py\n",
    "import os\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "\n",
    "\n",
    "#CNN with no dropout\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), activation='relu',\n",
    "input_shape=(150, 150, 3)))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout, Dense, Conv2D, MaxPooling2D, BatchNormalization\n",
    "from keras.layers.core import Flatten\n",
    "from keras.models import load_model\n",
    "from keras.activations import *\n",
    "from keras.optimizers import *\n",
    "#import talos\n",
    "#import wrangle\n",
    "import numpy as np\n",
    "import h5py\n",
    "import os\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "\n",
    "\n",
    "#cnn with dropout\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), activation='relu',\n",
    "input_shape=(150, 150, 3)))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compile model for binary image classification\n",
    "from keras import optimizers\n",
    "model.compile(loss='binary_crossentropy',\n",
    "optimizer=optimizers.RMSprop(lr=1e-4),\n",
    "metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "datagen = ImageDataGenerator(rescale=1./255)\n",
    "batch_size = 19\n",
    "\n",
    "def extract_features(directory, sample_count):\n",
    "    features = np.zeros(shape=(sample_count, 4, 4, 512))\n",
    "    labels = np.zeros(shape=(sample_count))\n",
    "    generator = datagen.flow_from_directory(\n",
    "        directory,\n",
    "        target_size=(150, 150),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='binary')\n",
    "    i = 0\n",
    "    for inputs_batch, labels_batch in generator:\n",
    "        features_batch = model.predict(inputs_batch)\n",
    "        features[i * batch_size : (i + 1) * batch_size] = features_batch\n",
    "        labels[i * batch_size : (i + 1) * batch_size] = labels_batch\n",
    "        i += 1\n",
    "        if i * batch_size >= sample_count:\n",
    "            break\n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = 'D:/Deep-Learning/base_dir/train/'\n",
    "validation_dir = 'D:/Deep-Learning/base_dir/validation/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features, train_labels = extract_features(train_dir, 1349)\n",
    "validation_features, validation_labels = extract_features(validation_dir, 1349)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features, test_labels = extract_features(test_dir, 378)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = np.reshape(train_features, (1349, 4 * 4 * 512))\n",
    "validation_features = np.reshape(validation_features, (1349, 4 * 4 * 512))\n",
    "test_features = np.reshape(test_features, (378, 4 * 4 * 512))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=40,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest')\n",
    "\n",
    "#dataset = tf.data.Dataset.list_files(\"/path/*.txt\") \n",
    "\n",
    "#train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "train_generator = datagen.flow_from_directory(\n",
    "    'D:/Deep-Learning/base_dir/train/',\n",
    "    target_size=(150, 150),\n",
    "    batch_size=19,\n",
    "    class_mode='binary')\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "    'D:/Deep-Learning/base_dir/validation/',\n",
    "    target_size=(150, 150),\n",
    "    batch_size=19,\n",
    "    class_mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=71,\n",
    "    epochs=30,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=71)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('2dconv.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "model = load_model('binary_image.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(1, len(acc) + 1)\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
